from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_ollama.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever
from IPython.display import display, Markdown
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever

def local_path(data):
loader = DirectoryLoader(data,
glob="*.pdf",
loader_cls=PyPDFLoader)
extracted_data = local_path("Data/")

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(extracted_data)
print(f"Text split into {len(chunks)} chunks")

# Create vector database
vector_db = Chroma.from_documents(
    documents=chunks,
    embedding=OllamaEmbeddings(model="nomic-embed-text"),
    collection_name="local-rag"
)
print("Vector database created successfully")

# Set up LLM and retrieval
local_model = "llama3.2"  # or whichever model you prefer
llm = ChatOllama(model=local_model)

# Query prompt template
QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an AI language model assistant. Your task is to generate 2
    different versions of the given user question to retrieve relevant documents from
    a vector database. By generating multiple perspectives on the user question, your
    goal is to help the user overcome some of the limitations of the distance-based
    similarity search. Provide these alternative questions separated by newlines.
    Original question: {question}""",
)

# Set up retriever
retriever = MultiQueryRetriever.from_llm(
    vector_db.as_retriever(), 
    llm,
    prompt=QUERY_PROMPT
)


template = """Some point to note while generating an answer:
a) Be curteous and creative in your responses. Answer like you are a friendly agent, making the user feel comfortable.
a.1) Imagine yourself as an intelligent assistant which is helping the users on different types of comapny policies. You will have a knolwedgebase of internal HR policies of a company to search on.
b) Be very structured in your response, Give the response with html tags as explained below
give <b> html tags for bullet points and to bold the important words. Use <br> html tags to display contents in new lines wherever required for clear displaying. 
Important Note : We need to display the content with proper HTML tags. Your task is to automatically add the relevant <li>, <ul>, break <br>, <p>, bold <b> tags to beautifully present the responses to be displayed in an html <div> tag.
c) Remember to not talk about any leaves that are not applicable to me. Example Maternity Leaves are not applicable to male employees. Contractual Leave is not valid for non contractual employeesS
d) The answer should not be in the email format and look like a normal chat.

Context:
{context}

Question: {question}

Answer:
Please provide the answer in a clear, point-wise format for better readability:
1. ...
2. ...
3. ...
"""

prompt = ChatPromptTemplate.from_template(template)

# Create chain
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

def chat_with_pdf(question):
    """
    Chat with the PDF using the RAG chain.
    """
    return display(Markdown(chain.invoke(question)))

# Example 1
chat_with_pdf("What is SQL?")